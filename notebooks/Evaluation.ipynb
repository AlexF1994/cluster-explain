{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c92dc0-e51c-45ed-9fde-c040e3d2ad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fottneal\\Documents\\code\\cluster-explain\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import mpl_toolkits.mplot3d \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cxplain.xkm import Xkm\n",
    "from cxplain.tree import  DecisionTreeExplainer, RandomForestExplainer, ExKMCExplainer\n",
    "from cxplain.shap import  ShapExplainer\n",
    "from cxplain.gradient import GradientExplainer  \n",
    "from cxplain.metrics import EuclideanMetric, Metric, ManhattenMetric\n",
    "from cxplain.neon import NeonKMeansExplainer\n",
    "from cxplain.errors import NonExistingRelevanceError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc6bf65-e310-4733-b642-7a0cb3bb21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalCKDE:\n",
    "    def __init__(self, data):\n",
    "        self.epsilon = 0.001\n",
    "        self.stopping_threshold = 100\n",
    "        self.data = data\n",
    "        self.variance = None\n",
    "        self.n_obs = self.data.shape[0]\n",
    "        self.n_features = self.data.shape[1]\n",
    "        \n",
    "    def fit(self):\n",
    "        counter = 0\n",
    "        variance = 1\n",
    "        variance_list =[variance]\n",
    "        \n",
    "        while not self._is_converged(variance_list):\n",
    "            variance_old = variance_list[-1]\n",
    "            variance_new = self._update_variance(variance_old)\n",
    "            variance_list.append(variance_new)\n",
    "            counter += 1\n",
    "            print(f\"counter: {counter}\")\n",
    "            if counter >= self.stopping_threshold:\n",
    "                print(f\"No convergence after {self.stopping_threshold} steps!\")\n",
    "                break\n",
    "                \n",
    "        self.variance = variance_list[-1]\n",
    "        \n",
    "    def _update_variance(self, variance_old):\n",
    "        update_weight = 1 / (self.n_obs * self.n_features)\n",
    "        observation_list = []\n",
    "        \n",
    "        for obs_index in range(self.n_obs):\n",
    "            base_obs = self.data[obs_index, :]\n",
    "            nominator = sum([np.exp(np.linalg.norm(base_obs - self.data[i, :]) / (2 * variance_old))\n",
    "                             * np.linalg.norm(base_obs - self.data[i, :]) \n",
    "                             for i in range(self.n_obs)\n",
    "                             if i != obs_index])\n",
    "            denominator = sum([np.exp(np.linalg.norm(base_obs - self.data[i, :]) / (2 * variance_old))\n",
    "                               for i in range(self.n_obs)\n",
    "                               if i != obs_index])\n",
    "            observation_list.append(nominator / denominator)\n",
    "            \n",
    "        return update_weight * sum(observation_list)\n",
    "    \n",
    "    def _is_converged(self, variance_list):\n",
    "        if len(variance_list) < 5:\n",
    "            return False\n",
    "        considered_elements = variance_list[-5:]\n",
    "        differences = np.diff(considered_elements)\n",
    "        return np.sum(differences >= self.epsilon) == 0\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1cc4244-4e6d-4213-b7bc-2eaa07ba22cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = NormalCKDE(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f60c41b9-5068-4710-90bc-b2e33ffcd769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter: 1\n",
      "counter: 2\n",
      "counter: 3\n",
      "counter: 4\n",
      "counter: 5\n",
      "counter: 6\n",
      "counter: 7\n",
      "counter: 8\n"
     ]
    }
   ],
   "source": [
    "imputer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6335be2-6a8c-43ee-8d6b-845703ae5916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9290669531414758"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer.variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad6dd3-77af-49c4-8947-8175309c2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_index = {\"iris\": }\n",
    "datasets = {\"iris\": }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1c97d-af83-4833-b895-578e6efa566f",
   "metadata": {},
   "source": [
    "First I only use the iris data set for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1d7d73c-b241-441c-b37d-8079059e8a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "n_obs = X.shape[0]\n",
    "n_features = X.shape[1]\n",
    "only_global = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3edb4ddf-2f38-47f3-972d-f98c5551ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit Kmeans\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=3).fit(X)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "predictions = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fcd1a507-2f46-4cc7-ae16-70576d97b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init and fit explainer\n",
    "# list allexplainers\n",
    "explainers = {\"tree\": DecisionTreeExplainer(data= X, cluster_predictions=predictions),\n",
    "             \"forest\": RandomForestExplainer(data= X, cluster_predictions=predictions),\n",
    "             \"exkmc\": ExKMCExplainer(X, kmeans, k=n_clusters, max_leaves=2*n_clusters),\n",
    "             \"gradient\": GradientExplainer(X, cluster_centers, predictions, EuclideanMetric, enable_abs_calculation=False),\n",
    "             \"shap\": ShapExplainer(data= X, cluster_predictions=predictions),\n",
    "             \"neon\": NeonKMeansExplainer(cluster_centers=cluster_centers, data=X, predictions=predictions),\n",
    "             \"xkm\": Xkm(data,  kmeans.cluster_centers_, \"euclidean\", predictions)}\n",
    "\n",
    "# fit and explain all explainers\n",
    "\n",
    "explanations = {explainer_name:explainer.fit_explain() for explainer_name, explainer  in explainers.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "560b5532-39ad-40cf-99b3-4ef44f6df731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first calculate all ROC curves for individual observations\n",
    "result_individual = {}\n",
    "for explainer_name, explanation in explanations.items():\n",
    "    # init curve_list\n",
    "    curve_list = []\n",
    "    for index_obs in range(n_obs):\n",
    "        # init list curve_obs_i to all 1 (length = num_features)\n",
    "        curve_obs = [1 for i in range(n_features)]\n",
    "        # init array of feature observations, I use an array instead of a list, as it is easier  later on to calculate distances to cluster centers\n",
    "        feature_obs = np.array([0.0 for i in range(n_features)])\n",
    "        # get relevance scores for observation, for explainers with only global scores, these will be used for every observation\n",
    "        if only_global:\n",
    "            relevance_scores = list(explanations[explainer_name].global_relevance)\n",
    "        else:\n",
    "            try:\n",
    "                relevance_scores = list(explanations[explainer_name].pointwise_relevance.iloc[index_obs, :])\n",
    "            except NonExistingRelevanceError:\n",
    "                relevance_scores = list(explanations[explainer_name].global_relevance)\n",
    "        \n",
    "        for feature_index in range(n_features):\n",
    "            # get biggest score and column index (indicate which feature is meant) and pop from list\n",
    "            index_biggest_score = relevance_scores.index(max(relevance_scores))                \n",
    "            relevance_scores[index_biggest_score] = -100 # I set to large negative number as popping would ruin the index correspondence from relevance score to feature\n",
    "            # get observation for this feature\n",
    "            obs_biggest_score = X[index_obs, index_biggest_score]\n",
    "            # get corresponding cluster index for this observation\n",
    "            cluster_index = predictions[index_obs]\n",
    "            # add observation for feature to feature observations list\n",
    "            feature_obs.put(index_biggest_score, obs_biggest_score) # has to be at index of feature in training data, as otherwise distance calculation is wrong\n",
    "            # impute other entries (length = num_features) --> TBD\n",
    "            # calculate distance to cluster centers for feature observations list\n",
    "            distances = [np.linalg.norm(feature_obs - center) for center in cluster_centers]\n",
    "            # get nearest_cluster_index\n",
    "            nearest_cluster_index = distances.index(min(distances))\n",
    "            # check whether cluster_index == nearest_cluster_index\n",
    "            # if yes: return curve_obs_i\n",
    "            # if no: replace first entry of curve_obs_i ith 0 and repeat\n",
    "            if cluster_index == nearest_cluster_index:\n",
    "                break\n",
    "            else:\n",
    "                curve_obs[feature_index] = 0\n",
    "            # if yes: return curve_obs_i\n",
    "            # if no: replace first entry of curve_obs_i ith 0 and repeat\n",
    "            \n",
    "        curve_list.append(curve_obs)\n",
    "        \n",
    "    # add explainer entry to dict\n",
    "    result_individual[explainer_name] = curve_list\n",
    "      \n",
    "# Now compute AUC\n",
    "result_auc = {explainer_name: (1 /(n_obs*n_features)) * sum(map(sum, curves)) for explainer_name, curves in result_individual.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dac639e1-4d2b-4077-94cf-a9f23d8bbddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tree': 0.7366666666666667,\n",
       " 'forest': 0.8016666666666667,\n",
       " 'exkmc': 0.7533333333333334,\n",
       " 'gradient': 0.7533333333333334,\n",
       " 'shap': 0.8016666666666667,\n",
       " 'neon': 0.6333333333333334,\n",
       " 'xkm': 0.8016666666666667}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cxplain",
   "language": "python",
   "name": "cxplain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
