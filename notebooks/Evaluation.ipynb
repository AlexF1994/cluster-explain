{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c92dc0-e51c-45ed-9fde-c040e3d2ad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fottneal\\Documents\\code\\cluster-explain\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn import datasets\n",
    "import mpl_toolkits.mplot3d \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cxplain.xkm import XkmExplainer\n",
    "from cxplain.tree import  DecisionTreeExplainer, RandomForestExplainer, ExKMCExplainer\n",
    "from cxplain.shap import  ShapExplainer\n",
    "from cxplain.gradient import GradientExplainer  \n",
    "from cxplain.metrics import EuclideanMetric, Metric, ManhattenMetric\n",
    "from cxplain.neon import NeonKMeansExplainer\n",
    "from cxplain.errors import NonExistingRelevanceError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc6bf65-e310-4733-b642-7a0cb3bb21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalCKDE:\n",
    "    def __init__(self, data):\n",
    "        self.epsilon = 0.00001\n",
    "        self.stopping_threshold = 100\n",
    "        self.data = data\n",
    "        self.variance = None\n",
    "        self.n_obs = self.data.shape[0]\n",
    "        self.n_features = self.data.shape[1]\n",
    "        \n",
    "    def fit(self):\n",
    "        counter = 0\n",
    "        variance = 1\n",
    "        variance_list =[variance]\n",
    "        \n",
    "        while not self._is_converged(variance_list):\n",
    "            variance_old = variance_list[-1]\n",
    "            variance_new = self._update_variance(variance_old)\n",
    "            variance_list.append(variance_new)\n",
    "            counter += 1\n",
    "            if counter >= self.stopping_threshold:\n",
    "                print(f\"No convergence after {self.stopping_threshold} steps!\")\n",
    "                break\n",
    "                \n",
    "        self.variance = variance_list[-1]\n",
    "        return self\n",
    "        \n",
    "    def _update_variance(self, variance_old):\n",
    "        update_weight = 1 / (self.n_obs * self.n_features)\n",
    "        observation_list = []\n",
    "        \n",
    "        for obs_index in range(self.n_obs):\n",
    "            base_obs = self.data[obs_index, :]\n",
    "            nominator = sum([np.exp(-np.linalg.norm(base_obs - self.data[i, :])**2 / (2 * variance_old))\n",
    "                             * np.linalg.norm(base_obs - self.data[i, :])**2 \n",
    "                             for i in range(self.n_obs)\n",
    "                             if i != obs_index])\n",
    "            denominator = sum([np.exp(-np.linalg.norm(base_obs - self.data[i, :])**2 / (2 * variance_old))\n",
    "                               for i in range(self.n_obs)\n",
    "                               if i != obs_index])\n",
    "            observation_list.append(nominator / denominator)\n",
    "            \n",
    "        return update_weight * sum(observation_list)\n",
    "    \n",
    "    def _is_converged(self, variance_list):\n",
    "        if len(variance_list) < 3:\n",
    "            return False\n",
    "        considered_elements = variance_list[-3:]\n",
    "        differences = np.diff(considered_elements)\n",
    "        return np.sum(differences >= self.epsilon) == 0\n",
    "                                                         \n",
    "    def predict(self, feature_observation):\n",
    "        rng = np.random.default_rng()\n",
    "        # convert feature observation to numpy array\n",
    "        feature_obs_arr = np.array(feature_observation, copy=True)\n",
    "        # calculate weights\n",
    "        # I first have to extract the indizes of given and to be imputed features\n",
    "        index_given = np.where(feature_obs_arr != 0)\n",
    "        index_impute = np.where(feature_obs_arr == 0) # I assume every obs to be imputed is 0\n",
    "        # now calculate weights with only given indizes\n",
    "        feature_obs_given = feature_obs_arr[index_given]\n",
    "        nominators = [np.exp(-np.linalg.norm(feature_obs_given - observation[index_given])**2 / (2 * self.variance))\n",
    "                      for observation in self.data]\n",
    "        denominators = []\n",
    "        for obs_index in range(self.n_obs):\n",
    "            denominator = [np.exp(-np.linalg.norm(feature_obs_given - self.data[i, :][index_given])**2 / (2 * self.variance))\n",
    "                           for i in range(self.n_obs)\n",
    "                           if i != obs_index]\n",
    "            denominators.append(sum(denominator))\n",
    "        weights = [nominator / denominator for nominator, denominator in zip(nominators, denominators)]\n",
    "        # sample index i  from weights distribution\n",
    "        distribution_index = random.choices(list(range(self.n_obs)), weights=weights, k=1)[0]\n",
    "        # sample from normal distribution i\n",
    "        # get observation of distribution_index\n",
    "        dist_obs = self.data[distribution_index, :]\n",
    "        # extract only features to be imputed\n",
    "        dist_obs_impute = dist_obs[index_impute]\n",
    "        # sample from MVN with mean = dist_obs_impute und variance self.variance * I\n",
    "        covariance = self.variance * np.identity(len(dist_obs_impute))\n",
    "        sample = rng.multivariate_normal(mean=dist_obs_impute,cov=covariance, size=1)\n",
    "        # fill up observation with imputed features\n",
    "        feature_obs_arr[index_impute] = sample\n",
    "        return feature_obs_arr\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcad6dd3-77af-49c4-8947-8175309c2968",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3084908807.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\fottneal\\AppData\\Local\\Temp\\ipykernel_21280\\3084908807.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    dataset_index = {\"iris\": }\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dataset_index = {\"iris\": }\n",
    "datasets = {\"iris\": }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1c97d-af83-4833-b895-578e6efa566f",
   "metadata": {},
   "source": [
    "First I only use the iris data set for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1d7d73c-b241-441c-b37d-8079059e8a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "n_obs = X.shape[0]\n",
    "n_features = X.shape[1]\n",
    "only_global = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3edb4ddf-2f38-47f3-972d-f98c5551ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit Kmeans\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=3).fit(X)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "predictions = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcd1a507-2f46-4cc7-ae16-70576d97b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init and fit explainer\n",
    "# list allexplainers\n",
    "explainers = {\"tree\": DecisionTreeExplainer(data= X, cluster_predictions=predictions),\n",
    "             \"forest\": RandomForestExplainer(data= X, cluster_predictions=predictions),\n",
    "             \"exkmc\": ExKMCExplainer(X, kmeans, k=n_clusters, max_leaves=2*n_clusters),\n",
    "             \"gradient\": GradientExplainer(X, cluster_centers, predictions, EuclideanMetric, enable_abs_calculation=False),\n",
    "             \"shap\": ShapExplainer(data= X, cluster_predictions=predictions),\n",
    "             \"neon\": NeonKMeansExplainer(cluster_centers=cluster_centers, data=X, predictions=predictions),\n",
    "             \"xkm_next_best\": XkmExplainer(X,  kmeans.cluster_centers_, \"next_best\", \"euclidean\", predictions),\n",
    "             \"xkm_all\": XkmExplainer(X,  kmeans.cluster_centers_, \"all\", \"euclidean\", predictions)}\n",
    "\n",
    "# fit and explain all explainers\n",
    "\n",
    "explanations = {explainer_name:explainer.fit_explain() for explainer_name, explainer  in explainers.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "560b5532-39ad-40cf-99b3-4ef44f6df731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first calculate all ROC curves for individual observations\n",
    "result_individual = {}\n",
    "imputer = NormalCKDE(X).fit()\n",
    "for explainer_name, explanation in explanations.items():\n",
    "    # init curve_list\n",
    "    curve_list = []\n",
    "    for index_obs in range(n_obs):\n",
    "        # init list curve_obs_i to all 1 (length = num_features)\n",
    "        curve_obs = [1 for i in range(n_features)]\n",
    "        # init array of feature observations, I use an array instead of a list, as it is easier  later on to calculate distances to cluster centers\n",
    "        feature_obs = np.array([0.0 for i in range(n_features)])\n",
    "        # get relevance scores for observation, for explainers with only global scores, these will be used for every observation\n",
    "        if only_global:\n",
    "            relevance_scores = list(explanations[explainer_name].global_relevance)\n",
    "        else:\n",
    "            try:\n",
    "                relevance_scores = list(explanations[explainer_name].pointwise_relevance.iloc[index_obs, :])\n",
    "            except NonExistingRelevanceError:\n",
    "                relevance_scores = list(explanations[explainer_name].global_relevance)\n",
    "        \n",
    "        for feature_index in range(n_features):\n",
    "            # get biggest score and column index (indicate which feature is meant) and pop from list\n",
    "            index_biggest_score = relevance_scores.index(max(relevance_scores))                \n",
    "            relevance_scores[index_biggest_score] = -100 # I set to large negative number as popping would ruin the index correspondence from relevance score to feature\n",
    "            # get observation for this feature\n",
    "            obs_biggest_score = X[index_obs, index_biggest_score]\n",
    "            # get corresponding cluster index for this observation\n",
    "            cluster_index = predictions[index_obs]\n",
    "            # add observation for feature to feature observations list\n",
    "            feature_obs.put(index_biggest_score, obs_biggest_score) # has to be at index of feature in training data, as otherwise distance calculation is wrong\n",
    "            # impute other entries (length = num_features) --> TBD\n",
    "            if feature_index < (n_features - 1):\n",
    "                feature_obs_imputed = imputer.predict(feature_obs)\n",
    "            else:\n",
    "                feature_obs_imputed = feature_obs.copy()\n",
    "            # calculate distance to cluster centers for feature observations list\n",
    "            distances = [np.linalg.norm(feature_obs_imputed - center) for center in cluster_centers]\n",
    "            # get nearest_cluster_index\n",
    "            nearest_cluster_index = distances.index(min(distances))\n",
    "            # check whether cluster_index == nearest_cluster_index\n",
    "            # if yes: return curve_obs_i\n",
    "            # if no: replace first entry of curve_obs_i ith 0 and repeat\n",
    "            if cluster_index == nearest_cluster_index:\n",
    "                break\n",
    "            else:\n",
    "                curve_obs[feature_index] = 0\n",
    "            # if yes: return curve_obs_i\n",
    "            # if no: replace first entry of curve_obs_i ith 0 and repeat\n",
    "            \n",
    "        curve_list.append(curve_obs)\n",
    "        \n",
    "    # add explainer entry to dict\n",
    "    result_individual[explainer_name] = curve_list\n",
    "      \n",
    "# Now compute AUC\n",
    "result_auc = {explainer_name: (1 /(n_obs*n_features)) * sum(map(sum, curves)) for explainer_name, curves in result_individual.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac639e1-4d2b-4077-94cf-a9f23d8bbddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tree': 0.9433333333333334,\n",
       " 'forest': 0.9533333333333334,\n",
       " 'exkmc': 0.9483333333333334,\n",
       " 'gradient': 0.8133333333333334,\n",
       " 'shap': 0.9450000000000001,\n",
       " 'neon': 0.7866666666666667,\n",
       " 'xkm_next_best': 0.9450000000000001,\n",
       " 'xkm_all': 0.9433333333333334}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cxplain",
   "language": "python",
   "name": "cxplain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
