{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49caa39-7d0a-45b2-99f0-60031ed20262",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "47c92dc0-e51c-45ed-9fde-c040e3d2ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod \n",
    "import random\n",
    "from numba import njit\n",
    "from numba.experimental import jitclass\n",
    "from numba import int32, float64\n",
    "from sklearn import datasets\n",
    "import mpl_toolkits.mplot3d \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cxplain.xkm import XkmExplainer\n",
    "from cxplain.tree import  DecisionTreeExplainer, RandomForestExplainer, ExKMCExplainer\n",
    "from cxplain.shap import  ShapExplainer\n",
    "from cxplain.gradient import GradientExplainer  \n",
    "from cxplain.metrics import EuclideanMetric, Metric, ManhattenMetric\n",
    "from cxplain.neon import NeonKMeansExplainer\n",
    "from cxplain.errors import NonExistingRelevanceError\n",
    "from imputer import NormalCKDEImputer, EmpiricalRandomImputer, get_imputer\n",
    "from datasets import IrisDataset, WineDataset, WholeSaleDataset, LiveSellersDataset, BuddyMoveDataset, SyntheticDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb9e41d-004a-4ae0-99cb-4496118b4e9a",
   "metadata": {},
   "source": [
    "Data sets to be considered:\n",
    "- https://archive-beta.ics.uci.edu/dataset/292/wholesale+customers\n",
    "- https://archive.ics.uci.edu/ml/datasets/BuddyMove+Data+Set# --> keine targets --> raus\n",
    "- https://archive.ics.uci.edu/ml/datasets/Facebook+Live+Sellers+in+Thailand#\n",
    "- https://archive.ics.uci.edu/ml/datasets/wine / https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine, included in scikit learn\n",
    "- https://www.researchgate.net/publication/331616284_A_morphological_database_for_Colombian_anuran_species_from_conservation-priority_ecosystems\n",
    "- https://archive.ics.uci.edu/ml/datasets/clickstream+data+for+online+shopping#\n",
    "- https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3edb4ddf-2f38-47f3-972d-f98c5551ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"iris\": IrisDataset.load_and_clean_dataset(),\n",
    "            \"wine\": WineDataset.load_and_clean_dataset(),\n",
    "            \"wholesale\": WholeSaleDataset.load_and_clean_dataset(\"../data/Wholesale customers data.csv\"),\n",
    "            \"buddy\": BuddyMoveDataset.load_and_clean_dataset(3, \"../data/buddymove_holidayiq.csv\"),\n",
    "            \"synthetic\": SyntheticDataset.load_and_clean_dataset(15, \"../data/data_s1.txt\"),\n",
    "            \"live_sellers\": LiveSellersDataset.load_and_clean_dataset(\"../data/Live_20210128.csv\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fcd1a507-2f46-4cc7-ae16-70576d97b5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "trial: 5\n",
      "trial: 6\n",
      "trial: 7\n",
      "trial: 8\n",
      "trial: 9\n",
      "trial: 10\n",
      "trial: 11\n",
      "trial: 12\n",
      "trial: 13\n",
      "trial: 14\n",
      "trial: 15\n",
      "trial: 16\n",
      "trial: 17\n",
      "trial: 18\n",
      "trial: 19\n",
      "trial: 20\n",
      "wine\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "trial: 5\n",
      "trial: 6\n",
      "trial: 7\n",
      "trial: 8\n",
      "trial: 9\n",
      "trial: 10\n",
      "trial: 11\n",
      "trial: 12\n",
      "trial: 13\n",
      "trial: 14\n",
      "trial: 15\n",
      "trial: 16\n",
      "trial: 17\n",
      "trial: 18\n",
      "trial: 19\n",
      "trial: 20\n",
      "wholesale\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "trial: 5\n",
      "trial: 6\n",
      "trial: 7\n",
      "trial: 8\n",
      "trial: 9\n",
      "trial: 10\n",
      "trial: 11\n",
      "trial: 12\n",
      "trial: 13\n",
      "trial: 14\n",
      "trial: 15\n",
      "trial: 16\n",
      "trial: 17\n",
      "trial: 18\n",
      "trial: 19\n",
      "trial: 20\n",
      "buddy\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "trial: 5\n",
      "trial: 6\n",
      "trial: 7\n",
      "trial: 8\n",
      "trial: 9\n",
      "trial: 10\n",
      "trial: 11\n",
      "trial: 12\n",
      "trial: 13\n",
      "trial: 14\n",
      "trial: 15\n",
      "trial: 16\n",
      "trial: 17\n",
      "trial: 18\n",
      "trial: 19\n",
      "trial: 20\n",
      "synthetic\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "trial: 5\n",
      "trial: 6\n",
      "trial: 7\n",
      "trial: 8\n",
      "trial: 9\n",
      "trial: 10\n",
      "trial: 11\n",
      "trial: 12\n",
      "trial: 13\n",
      "trial: 14\n",
      "trial: 15\n",
      "trial: 16\n",
      "trial: 17\n",
      "trial: 18\n",
      "trial: 19\n",
      "trial: 20\n",
      "live_sellers\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "trial: 5\n",
      "trial: 6\n",
      "trial: 7\n",
      "trial: 8\n",
      "trial: 9\n",
      "trial: 10\n",
      "trial: 11\n",
      "trial: 12\n",
      "trial: 13\n",
      "trial: 14\n",
      "trial: 15\n",
      "trial: 16\n",
      "trial: 17\n",
      "trial: 18\n",
      "trial: 19\n",
      "trial: 20\n"
     ]
    }
   ],
   "source": [
    "only_global = True\n",
    "use_imputer = True\n",
    "imputer_name = \"empirical\"\n",
    "n_trials = 20\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print(dataset_name)\n",
    "\n",
    "    n_clusters = dataset.n_clusters\n",
    "    X = dataset.features\n",
    "    y = dataset.targets\n",
    "    n_obs = dataset.n_obs\n",
    "    n_features = dataset.n_features\n",
    "\n",
    "    # fit Kmeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=3).fit(X)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    predictions = kmeans.predict(X)\n",
    "    # init and fit explainer\n",
    "    # list allexplainers\n",
    "    explainers = {\"tree\": DecisionTreeExplainer(data= X, cluster_predictions=predictions),\n",
    "                 \"forest\": RandomForestExplainer(data= X, cluster_predictions=predictions),\n",
    "                 \"exkmc\": ExKMCExplainer(X, kmeans, k=n_clusters, max_leaves=2*n_clusters),\n",
    "                 \"gradient\": GradientExplainer(X, cluster_centers, predictions, EuclideanMetric, enable_abs_calculation=False),\n",
    "                 \"shap\": ShapExplainer(data= X, cluster_predictions=predictions),\n",
    "                 \"neon\": NeonKMeansExplainer(cluster_centers=cluster_centers, data=X, predictions=predictions),\n",
    "                 \"xkm_next_best\": XkmExplainer(X,  kmeans.cluster_centers_, \"next_best\", \"euclidean\", predictions),\n",
    "                 \"xkm_all\": XkmExplainer(X,  kmeans.cluster_centers_, \"all\", \"euclidean\", predictions)}\n",
    "\n",
    "    # fit and explain all explainers\n",
    "\n",
    "    explanations = {explainer_name:explainer.fit_explain() for explainer_name, explainer  in explainers.items()}\n",
    "\n",
    "    # first calculate all ROC curves for individual observations\n",
    "    result_individual = {explainer_name: [] for explainer_name in explanations.keys()}\n",
    "    if use_imputer:\n",
    "        imputer = get_imputer(imputer_name)(X).fit()\n",
    "    for _ in range(n_trials):\n",
    "        print(f\"trial: {_ + 1}\")\n",
    "        for explainer_name, explanation in explanations.items():\n",
    "            # init curve_list\n",
    "            curve_list = []\n",
    "            for index_obs in range(n_obs):\n",
    "                # init list curve_obs_i to all 1 (length = num_features)\n",
    "                curve_obs = [1 for i in range(n_features)]\n",
    "                # init array of feature observations, I use an array instead of a list, as it is easier  later on to calculate distances to cluster centers\n",
    "                feature_obs = np.array([0.0 for i in range(n_features)])\n",
    "                # get relevance scores for observation, for explainers with only global scores, these will be used for every observation\n",
    "                if only_global:\n",
    "                    relevance_scores = list(explanations[explainer_name].global_relevance)\n",
    "                else:\n",
    "                    try:\n",
    "                        relevance_scores = list(explanations[explainer_name].pointwise_relevance.iloc[index_obs, :])\n",
    "                    except NonExistingRelevanceError:\n",
    "                        relevance_scores = list(explanations[explainer_name].global_relevance)\n",
    "\n",
    "                for feature_index in range(n_features):\n",
    "                    # get biggest score and column index (indicate which feature is meant) and pop from list\n",
    "                    index_biggest_score = relevance_scores.index(max(relevance_scores))                \n",
    "                    relevance_scores[index_biggest_score] = -100 # I set to large negative number as popping would ruin the index correspondence from relevance score to feature\n",
    "                    # get observation for this feature\n",
    "                    obs_biggest_score = X[index_obs, index_biggest_score]\n",
    "                    # get corresponding cluster index for this observation\n",
    "                    cluster_index = predictions[index_obs]\n",
    "                    # add observation for feature to feature observations list\n",
    "                    feature_obs.put(index_biggest_score, obs_biggest_score) # has to be at index of feature in training data, as otherwise distance calculation is wrong\n",
    "                    # impute other entries (length = num_features) --> TBD\n",
    "                    if use_imputer: \n",
    "                        if feature_index < (n_features - 1):\n",
    "                            feature_obs_imputed = imputer.predict(feature_obs, index_obs)\n",
    "                        else:\n",
    "                            feature_obs_imputed = feature_obs.copy()\n",
    "                    else:\n",
    "                        feature_obs_imputed = feature_obs.copy()\n",
    "                    # calculate distance to cluster centers for feature observations list\n",
    "                    distances = [np.linalg.norm(feature_obs_imputed - center) for center in cluster_centers]\n",
    "                    # get nearest_cluster_index\n",
    "                    nearest_cluster_index = distances.index(min(distances))\n",
    "                    # check whether cluster_index == nearest_cluster_index\n",
    "                    # if yes: return curve_obs_i\n",
    "                    # if no: replace first entry of curve_obs_i ith 0 and repeat\n",
    "                    if cluster_index == nearest_cluster_index:\n",
    "                        break\n",
    "                    else:\n",
    "                        curve_obs[feature_index] = 0\n",
    "                    # if yes: return curve_obs_i\n",
    "                    # if no: replace first entry of curve_obs_i ith 0 and repeat\n",
    "\n",
    "                curve_list.append(curve_obs)\n",
    "\n",
    "            # add explainer entry to dict\n",
    "            result_individual[explainer_name].extend(curve_list)\n",
    "\n",
    "    # Now compute AUC\n",
    "    result_auc = {explainer_name: (1 /(n_obs*n_features*n_trials)) * sum(map(sum, curves)) for explainer_name, curves in result_individual.items()}\n",
    "\n",
    "    dataset_results[dataset_name] = result_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dac639e1-4d2b-4077-94cf-a9f23d8bbddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iris': {'tree': 0.8262499999999999,\n",
       "  'forest': 0.9472499999999999,\n",
       "  'exkmc': 0.8336666666666667,\n",
       "  'gradient': 0.8019166666666666,\n",
       "  'shap': 0.9486666666666667,\n",
       "  'neon': 0.7834166666666667,\n",
       "  'xkm_next_best': 0.8650833333333333,\n",
       "  'xkm_all': 0.8718333333333333},\n",
       " 'wine': {'tree': 0.9346585998271392,\n",
       "  'forest': 0.9357173725151253,\n",
       "  'exkmc': 0.8927830596369922,\n",
       "  'gradient': 0.9031114952463267,\n",
       "  'shap': 0.9550561797752809,\n",
       "  'neon': 0.8845505617977528,\n",
       "  'xkm_next_best': 0.9285220397579947,\n",
       "  'xkm_all': 0.9226231633535004},\n",
       " 'wholesale': {'tree': 0.9460037878787879,\n",
       "  'forest': 0.9478030303030303,\n",
       "  'exkmc': 0.9132386363636363,\n",
       "  'gradient': 0.8964015151515151,\n",
       "  'shap': 0.9682954545454545,\n",
       "  'neon': 0.9400946969696969,\n",
       "  'xkm_next_best': 0.9323484848484849,\n",
       "  'xkm_all': 0.9004545454545455},\n",
       " 'live_sellers': {'tree': 0.9459708431836091,\n",
       "  'forest': 0.9523333333333334,\n",
       "  'exkmc': 0.9457391646966116,\n",
       "  'gradient': 0.9308242710795902,\n",
       "  'shap': 0.9645949566587865,\n",
       "  'neon': 0.9275421591804571,\n",
       "  'xkm_next_best': 0.9398400315208826,\n",
       "  'xkm_all': 0.9005334909377463},\n",
       " 'buddy': {'tree': 0.9032128514056226,\n",
       "  'forest': 0.8981927710843374,\n",
       "  'exkmc': 0.8898594377510041,\n",
       "  'gradient': 0.8346720214190094,\n",
       "  'shap': 0.9214524765729586,\n",
       "  'neon': 0.8039825970548863,\n",
       "  'xkm_next_best': 0.894277108433735,\n",
       "  'xkm_all': 0.8744645247657297},\n",
       " 'synthetic': {'tree': 0.62773,\n",
       "  'forest': 0.62785,\n",
       "  'exkmc': 0.6369,\n",
       "  'gradient': 0.63263,\n",
       "  'shap': 0.644785,\n",
       "  'neon': 0.626765,\n",
       "  'xkm_next_best': 0.6327250000000001,\n",
       "  'xkm_all': 0.6355850000000001}}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cxplain",
   "language": "python",
   "name": "cxplain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
